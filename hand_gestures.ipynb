{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rock/Paper/Scissors Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset in\n",
    "TRAINING_DIR = \"/Users/medhaniesolomon/programming/VS code/ai_coders/rps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR = \"/Users/medhaniesolomon/programming/VS code/ai_coders/rps-test-set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images into image data generator which augments it\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n",
      "Found 2520 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# use more than 2 subdirectories\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # input shape is the desired size of the image: 150 x 150 x 3\n",
    "    # first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu',\n",
    "                           input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu',\n",
    "                           input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu',\n",
    "                           input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # forth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu',\n",
    "                           input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # 3 classes\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 19:35:44.815298: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 18s 17s/step - loss: 0.9287 - accuracy: 0.5469 - val_loss: 0.7748 - val_accuracy: 0.6944\n",
      "Epoch 2/25\n",
      "2/2 [==============================] - 18s 18s/step - loss: 0.7434 - accuracy: 0.6250 - val_loss: 0.6831 - val_accuracy: 0.7476\n",
      "Epoch 3/25\n",
      "2/2 [==============================] - 18s 18s/step - loss: 0.7016 - accuracy: 0.7969 - val_loss: 0.8511 - val_accuracy: 0.5889\n",
      "Epoch 4/25\n",
      "2/2 [==============================] - 19s 18s/step - loss: 0.6831 - accuracy: 0.6094 - val_loss: 0.8366 - val_accuracy: 0.5817\n",
      "Epoch 5/25\n",
      "2/2 [==============================] - 19s 18s/step - loss: 1.0146 - accuracy: 0.5469 - val_loss: 0.7146 - val_accuracy: 0.7274\n",
      "Epoch 6/25\n",
      "2/2 [==============================] - 19s 18s/step - loss: 0.6592 - accuracy: 0.7679 - val_loss: 0.6774 - val_accuracy: 0.6992\n",
      "Epoch 7/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.8414 - accuracy: 0.6562 - val_loss: 0.9490 - val_accuracy: 0.5409\n",
      "Epoch 8/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.8325 - accuracy: 0.5469 - val_loss: 0.7526 - val_accuracy: 0.6810\n",
      "Epoch 9/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.6148 - accuracy: 0.7812 - val_loss: 0.6300 - val_accuracy: 0.7377\n",
      "Epoch 10/25\n",
      "2/2 [==============================] - 19s 18s/step - loss: 0.6327 - accuracy: 0.7031 - val_loss: 0.7313 - val_accuracy: 0.6429\n",
      "Epoch 11/25\n",
      "2/2 [==============================] - 19s 18s/step - loss: 0.7036 - accuracy: 0.6875 - val_loss: 0.7249 - val_accuracy: 0.6913\n",
      "Epoch 12/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.7665 - accuracy: 0.6562 - val_loss: 0.7058 - val_accuracy: 0.7119\n",
      "Epoch 13/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.5471 - accuracy: 0.7812 - val_loss: 0.6807 - val_accuracy: 0.6504\n",
      "Epoch 14/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.8700 - accuracy: 0.5000 - val_loss: 0.7204 - val_accuracy: 0.7187\n",
      "Epoch 15/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.6571 - accuracy: 0.7679 - val_loss: 0.5642 - val_accuracy: 0.7841\n",
      "Epoch 16/25\n",
      "2/2 [==============================] - 19s 19s/step - loss: 5.4579 - accuracy: 0.5536 - val_loss: 0.9106 - val_accuracy: 0.5980\n",
      "Epoch 17/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 1.0221 - accuracy: 0.5781 - val_loss: 0.9560 - val_accuracy: 0.5611\n",
      "Epoch 18/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.8324 - accuracy: 0.6094 - val_loss: 0.8158 - val_accuracy: 0.6381\n",
      "Epoch 19/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.7776 - accuracy: 0.5781 - val_loss: 0.7938 - val_accuracy: 0.6532\n",
      "Epoch 20/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.9819 - accuracy: 0.5938 - val_loss: 0.7884 - val_accuracy: 0.6627\n",
      "Epoch 21/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.6549 - accuracy: 0.7969 - val_loss: 0.8077 - val_accuracy: 0.6536\n",
      "Epoch 22/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.7363 - accuracy: 0.6719 - val_loss: 0.7001 - val_accuracy: 0.7123\n",
      "Epoch 23/25\n",
      "2/2 [==============================] - 19s 19s/step - loss: 0.6062 - accuracy: 0.7188 - val_loss: 0.7147 - val_accuracy: 0.6619\n",
      "Epoch 24/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.6933 - accuracy: 0.6719 - val_loss: 0.6135 - val_accuracy: 0.7679\n",
      "Epoch 25/25\n",
      "2/2 [==============================] - 20s 19s/step - loss: 0.6861 - accuracy: 0.6875 - val_loss: 0.6066 - val_accuracy: 0.7433\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_generator, epochs=25,\n",
    "                    steps_per_epoch=2,\n",
    "                    validation_data = validation_generator, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout regularization to make sure network isnt overfitting or overspecializing to training data\n",
    "# tf.keras.layers.Dropout(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
