{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horse or Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import zipfile\n",
    "\n",
    "local_zip = \"/Users/medhaniesolomon/Downloads/horse-or-human.zip\"\n",
    "\n",
    "file_name = \"horse-or-human.zip\"\n",
    "training_dir = 'horse-or-human/training/'\n",
    "\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall(training_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate images for training images\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(300, 300),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architecture\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                           input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 11:20:58.134889: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 580ms/step - loss: 0.7296 - accuracy: 0.6523\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 4s 501ms/step - loss: 0.7875 - accuracy: 0.6784\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 5s 607ms/step - loss: 0.3975 - accuracy: 0.8359\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 5s 565ms/step - loss: 0.4711 - accuracy: 0.8320\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 5s 554ms/step - loss: 0.3620 - accuracy: 0.8828\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 0.3117 - accuracy: 0.8906\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 5s 608ms/step - loss: 0.1727 - accuracy: 0.9453\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 0.3747 - accuracy: 0.8722\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 5s 590ms/step - loss: 0.1529 - accuracy: 0.9414\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 6s 681ms/step - loss: 0.2893 - accuracy: 0.9297\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 5s 593ms/step - loss: 0.1151 - accuracy: 0.9609\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 5s 604ms/step - loss: 0.1335 - accuracy: 0.9453\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 5s 635ms/step - loss: 0.1185 - accuracy: 0.9609\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 5s 608ms/step - loss: 0.0434 - accuracy: 0.9766\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 5s 556ms/step - loss: 0.1337 - accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance of model with validation set\n",
    "local_zip = \"/Users/medhaniesolomon/Downloads/validation-horse-or-human.zip\"\n",
    "\n",
    "file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = 'horse-or-human/validation/'\n",
    "\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall(validation_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create image generator\n",
    "# All images will be rescaled by 1./255\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(300, 300),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 11:22:42.340473: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9471"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 11:22:47.084524: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 809ms/step - loss: 0.1843 - accuracy: 0.9471 - val_loss: 1.3000 - val_accuracy: 0.8477\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 7s 849ms/step - loss: 0.0498 - accuracy: 0.9805 - val_loss: 1.8847 - val_accuracy: 0.8438\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 6s 758ms/step - loss: 0.1884 - accuracy: 0.9515 - val_loss: 2.1568 - val_accuracy: 0.7148\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 7s 856ms/step - loss: 0.0906 - accuracy: 0.9688 - val_loss: 1.1609 - val_accuracy: 0.8438\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 7s 834ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 1.4082 - val_accuracy: 0.8555\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 7s 939ms/step - loss: 0.0214 - accuracy: 0.9961 - val_loss: 2.9601 - val_accuracy: 0.7617\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 7s 924ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.2609 - val_accuracy: 0.8164\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 7s 910ms/step - loss: 0.4169 - accuracy: 0.9336 - val_loss: 2.2745 - val_accuracy: 0.8242\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 7s 893ms/step - loss: 0.0723 - accuracy: 0.9805 - val_loss: 1.0089 - val_accuracy: 0.8867\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 7s 831ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.2961 - val_accuracy: 0.8594\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 7s 828ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3322 - val_accuracy: 0.8906\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 7s 958ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.0622 - val_accuracy: 0.8633\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 7s 903ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 1.5365 - val_accuracy: 0.6562\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 7s 925ms/step - loss: 0.3166 - accuracy: 0.9180 - val_loss: 1.4516 - val_accuracy: 0.8555\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 0.0102 - accuracy: 0.9956 - val_loss: 2.2668 - val_accuracy: 0.7734\n"
     ]
    }
   ],
   "source": [
    "# perform validation\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "[1.168124e-08]\n",
      "/Users/medhaniesolomon/.keras/datasets/Jkramerheadshot-scaled-e1645036825432-1050x1050-c-default.jpg is a horse\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from google.colab import files\n",
    "import keras.utils as image\n",
    "\n",
    "file = tf.keras.utils.get_file(\"Jkramerheadshot-scaled-e1645036825432-1050x1050-c-default.jpg\", origin=\"https://www.georgetown.edu/wp-content/uploads/2022/02/Jkramerheadshot-scaled-e1645036825432-1050x1050-c-default.jpg\")\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for fn in uploaded.keys():\n",
    "\n",
    "# for fn in files:\n",
    "\n",
    "  # predicting images\n",
    "  # path = '/content/' + fn\n",
    "img = image.load_img(path=file, target_size=(300, 300))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "classes = model.predict(images, batch_size=10)\n",
    "print(classes[0])\n",
    "if classes[0]>0.5:\n",
    "  print(file + \" is a human\")\n",
    "else:\n",
    "  print(file + \" is a horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
